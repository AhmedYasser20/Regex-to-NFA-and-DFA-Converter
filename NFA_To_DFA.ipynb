{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKYWUZIZULfU"
      },
      "outputs": [],
      "source": [
        "\n",
        "EPSION= 'ESP'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSxznPTCNspo"
      },
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "def vizualize_NFA(NFA):\n",
        "\n",
        "  #gra = Digraph(graph_attr={'landscape':'True'})\n",
        "  gra = Digraph(graph_attr={'rankdir':'LR'})\n",
        "\n",
        "\n",
        "  #construct nodes first\n",
        "  for stat in NFA:\n",
        "\n",
        "     if \"startingState\" in stat:\n",
        "        continue\n",
        "     for key, value in NFA[stat].items():\n",
        "        if key == 'isTerminatingState':\n",
        "          if value==True:\n",
        "            gra.node(stat, _attributes={'peripheries' : '2'})\n",
        "          else:\n",
        "            gra.node(stat)\n",
        "\n",
        "  #for each node, construct edges\n",
        "  for stat in NFA:\n",
        "      if \"startingState\" in stat:\n",
        "\n",
        "        continue\n",
        "      for edg,values in NFA[stat].items():\n",
        "          if(edg !=\"isTerminatingState\"):\n",
        "            if(isinstance(values,list)):\n",
        "              for value in values:\n",
        "                gra.edge(stat, value, edg)\n",
        "            else:\n",
        "                gra.edge(stat, values, edg)\n",
        "  gra.node('', shape='none')\n",
        "  gra.edge('',NFA['startingState'], label='Start')\n",
        "  gra.format = 'png'\n",
        "  gra.render('NFA', view = True)\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDO-wAi9NXn2"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from enum import Enum\n",
        "class RegexErrors(Enum):\n",
        "    SUCCESS = 0\n",
        "    UNALLOWED_CHAR = 1\n",
        "    INVALID_DASH = 2\n",
        "    INVALID_BRACKETS = 3\n",
        "    INVALID_OR = 4\n",
        "    INVALID_INITIAL = 5\n",
        "    EMPTY_BRACKET = 6\n",
        "\n",
        "\n",
        "def is_valid_dash(s:str,i:int) -> bool:\n",
        "    if (s[i - 1].isalnum() and s[i + 1].isalnum()):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def is_valid_bar(s:str,i:int) -> bool:\n",
        "    if (s[i - 1].isalnum() or s[i-1] == ')'or s[i+1] == ']') and (s[i + 1].isalnum() or s[i+1] == '(' or s[i+1] == '[' ):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def is_allowed_char (ch: str, allowed_non_alnum: Tuple[str, ...]) -> bool:\n",
        "    if ch not in allowed_non_alnum and not (ch.isalnum()):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def is_empty_bracket(s:str,i) -> bool:\n",
        "    if (s[i] == '[' and s[i+1] == ']' or s[i] == '(' and s[i+1] == ')' ):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def is_valid_first_char(s:str) -> bool:\n",
        "    if (not (is_allowed_char(s[0],allowed_non_alnum))):\n",
        "        return RegexErrors.UNALLOWED_CHAR\n",
        "    # if (not (s[0].isalnum() or s[0] =='(' )):\n",
        "\n",
        "\n",
        "def is_valid_regex(s: str, allowed_non_alnum: Tuple[str, ...]) -> RegexErrors:\n",
        "    stack: List[str] = []\n",
        "    stack_square_brackets: List[str] = []\n",
        "    brackets_map: dict[str, str] = {')': '(',}\n",
        "    square_brackets_map: dict[str, str] = {']': '['}\n",
        "    last_index = len(s) - 1\n",
        "    #hardcoding 1st iteration due to index error from dashing\n",
        "    if not is_allowed_char(s[0], allowed_non_alnum):\n",
        "        return RegexErrors.UNALLOWED_CHAR\n",
        "    elif not (s[0].isalnum() or s[0] =='(' or s[0] == '['):\n",
        "        return RegexErrors.INVALID_INITIAL\n",
        "    if s[0] in brackets_map.values():\n",
        "        if is_empty_bracket(s,0):\n",
        "            return RegexErrors.EMPTY_BRACKET\n",
        "        stack.append(s[0])\n",
        "    elif s[0] in square_brackets_map.values():\n",
        "        if is_empty_bracket(s,0):\n",
        "            return RegexErrors.EMPTY_BRACKET\n",
        "        stack_square_brackets.append(s[0])\n",
        "    #Looping through the rest of the string\n",
        "    for i, char in enumerate(s[1:last_index], start=1):\n",
        "        if not (is_allowed_char(char,allowed_non_alnum)):\n",
        "            return RegexErrors.UNALLOWED_CHAR\n",
        "        if char == '-' and (not is_valid_dash(s,i) or not stack_square_brackets):\n",
        "            return RegexErrors.INVALID_DASH\n",
        "        elif char == '|' and not is_valid_bar(s,i):\n",
        "            return RegexErrors.INVALID_OR\n",
        "        elif char in brackets_map.values():\n",
        "            if stack_square_brackets:\n",
        "                return RegexErrors.INVALID_BRACKETS\n",
        "            if is_empty_bracket(s,i):\n",
        "                return RegexErrors.EMPTY_BRACKET\n",
        "            stack.append(char)\n",
        "        elif char in square_brackets_map.values():\n",
        "            if is_empty_bracket(s,i):\n",
        "                return RegexErrors.EMPTY_BRACKET\n",
        "            stack_square_brackets.append(char)\n",
        "        elif char in brackets_map.keys():\n",
        "            if not stack or brackets_map[char] != stack.pop():\n",
        "                return RegexErrors.INVALID_BRACKETS\n",
        "        elif char in square_brackets_map.keys():\n",
        "            if not stack_square_brackets or square_brackets_map[char] != stack_square_brackets.pop():\n",
        "                return RegexErrors.INVALID_BRACKETS\n",
        "    #hardcoding last iteration due to index error from dashing\n",
        "    if s[-1] == '|':\n",
        "        return RegexErrors.INVALID_OR\n",
        "    if s[-1] not in allowed_non_alnum and not (s[-1].isalnum()):\n",
        "            return RegexErrors.UNALLOWED_CHAR\n",
        "    if s[-1] in brackets_map.values():\n",
        "        return RegexErrors.INVALID_BRACKETS\n",
        "    elif s[-1] in brackets_map.keys():\n",
        "        if not stack or brackets_map[s[-1]] != stack.pop():\n",
        "            return RegexErrors.INVALID_BRACKETS\n",
        "    if (stack or stack_square_brackets):\n",
        "      return RegexErrors.INVALID_BRACKETS\n",
        "    return RegexErrors.SUCCESS\n",
        "    # return RegexErrors.SUCCESS if not (stack or stack_square_brackets)  else RegexErrors.INVALID_BRACKETS\n",
        "\n",
        "from typing import Dict, Union\n",
        "import json\n",
        "\n",
        "class Node:\n",
        "    state_id_counter: int = 1  # Static data member to calculate state_id\n",
        "\n",
        "    def __init__(self, is_start: bool = False, is_end: bool = False):\n",
        "        self.is_start: bool = is_start\n",
        "        self.is_end: bool = is_end\n",
        "        self.state_id: str = f\"S{Node.state_id_counter}\"  # State ID for the node\n",
        "        Node.state_id_counter += 1\n",
        "        self.connections: Dict[str, Union[int, list]] = {}  # Dictionary to store connections to other nodes\n",
        "\n",
        "    def add_connection(self, to_node, action_value: str) -> None:\n",
        "        if action_value in self.connections:\n",
        "            # If action already exists, append the new destination node to the existing one\n",
        "            existing_destination = self.connections[action_value]\n",
        "            if isinstance(existing_destination, list):\n",
        "                existing_destination.append(to_node.state_id)\n",
        "            else:\n",
        "                self.connections[action_value] = [existing_destination, to_node.state_id]\n",
        "        else:\n",
        "            self.connections[action_value] = to_node.state_id\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        connections_dict = {}\n",
        "        for action, destination in self.connections.items():\n",
        "            connections_dict[str(action)] = destination\n",
        "        state_dict = {\n",
        "                \"isTerminatingState\": self.is_end,\n",
        "        }\n",
        "        state_dict.update(connections_dict)\n",
        "        return state_dict\n",
        "\n",
        "\n",
        "    def remove_connection(self, action_value: str) -> None:\n",
        "        if action_value in self.connections:\n",
        "            del self.connections[action_value]\n",
        "        else:\n",
        "            print(\"Connection with action value {} does not exist.\".format(action_value))\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.nodes: List[Node] = []\n",
        "\n",
        "    def add_node(self, node: Node) -> None:\n",
        "        self.nodes.append(node)\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            node.state_id: node.to_dict() for node in self.nodes\n",
        "        }\n",
        "\n",
        "global_graph: Graph = Graph()\n",
        "\n",
        "class Token:\n",
        "    def __init__(self, start_node: Node = None, end_node: Node = None):\n",
        "        self.start_node: Node = start_node\n",
        "        self.end_node: Node = end_node\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'startingState': self.start_node.state_id\n",
        "        }\n",
        "\n",
        "def construct_literal(token: Token, action: str) -> Token:\n",
        "    new_node: Node = Node(is_end=True)\n",
        "    global_graph.add_node(new_node)\n",
        "    token.end_node.add_connection(new_node, action_value=action)\n",
        "    token.end_node.is_end = False\n",
        "    token.end_node = new_node\n",
        "    return token\n",
        "\n",
        "def construct_literal_optional(token: Token) -> Token:\n",
        "    new_start: Node = Node(is_start=True)\n",
        "    global_graph.add_node(new_start)\n",
        "    new_start.add_connection(token.start_node, 'ESP')\n",
        "    token.start_node.is_start = False\n",
        "    token.start_node = new_start\n",
        "    new_end_node: Node = Node(is_end=True)\n",
        "    global_graph.add_node(new_end_node)\n",
        "    token.end_node.add_connection(new_end_node, 'ESP')\n",
        "    token.end_node.is_end = False\n",
        "    token.end_node = new_end_node\n",
        "    token.start_node.add_connection(token.end_node, 'ESP')\n",
        "    return token\n",
        "\n",
        "def construct_literal_kleene_star(token: Token) -> Token:\n",
        "    new_start: Node = Node(is_start=True)\n",
        "    global_graph.add_node(new_start)\n",
        "    new_start.add_connection(token.start_node, 'ESP')\n",
        "    token.start_node.is_start = False\n",
        "    token.start_node = new_start #line 1-5/part 1, highlighted in red\n",
        "    token.end_node.add_connection(new_start, 'ESP') #line 6/part 2, highlighted in green\n",
        "    new_end_node: Node = Node(is_end=True)\n",
        "    global_graph.add_node(new_end_node)\n",
        "    token.end_node.add_connection(new_end_node, 'ESP')\n",
        "    token.end_node.is_end = False\n",
        "    token.end_node = new_end_node\n",
        "    token.start_node.add_connection(token.end_node, 'ESP')\n",
        "    return token\n",
        "\n",
        "def construct_literal_kleene_plus(token: Token) -> Token:\n",
        "    new_start: Node = Node(is_start=True)\n",
        "    global_graph.add_node(new_start)\n",
        "    new_start.add_connection(token.start_node, 'ESP')\n",
        "    token.start_node.is_start = False\n",
        "    token.start_node = new_start\n",
        "    token.end_node.add_connection(new_start, 'ESP')\n",
        "    return token\n",
        "\n",
        "def construct_base(base: str) -> Token:\n",
        "    start_node: Node = Node(is_start=True)\n",
        "    global_graph.add_node(start_node)\n",
        "    end_node: Node = Node(is_end=True)\n",
        "    global_graph.add_node(end_node)\n",
        "    token: Token = Token(start_node=start_node, end_node=end_node)\n",
        "    start_node.add_connection(end_node, base)\n",
        "    return token\n",
        "\n",
        "def add_quantifier(token: Token, ch: str) -> Tuple[Token,bool]:\n",
        "    has_quantifier = True\n",
        "    if(ch == '?'):\n",
        "       token = construct_literal_optional(token=token)\n",
        "    elif(ch == '*'):\n",
        "       token = construct_literal_kleene_star(token=token)\n",
        "    elif(ch == '+'):\n",
        "        token = construct_literal_kleene_plus(token=token)\n",
        "    else:\n",
        "        has_quantifier = False\n",
        "    return token,has_quantifier\n",
        "def and_tokens(token1: Token, token2: Token) -> Token:\n",
        "    token1.end_node.add_connection(token2.start_node,'ESP')\n",
        "    token1.end_node = token2.end_node\n",
        "    token2.start_node.is_start = False\n",
        "    token1.end_node.is_end = False\n",
        "    token2.end_node.is_end = True\n",
        "    return token1\n",
        "\n",
        "def or_tokens(token1: Token, token2: Token) -> Token:\n",
        "    new_start_node: Node = Node(is_start=True)\n",
        "    global_graph.add_node(new_start_node)\n",
        "    new_start_node.add_connection(token1.start_node, 'ESP')\n",
        "    new_start_node.add_connection(token2.start_node, 'ESP')\n",
        "    token1.start_node.is_start = False\n",
        "    token2.start_node.is_start = False\n",
        "    token1.start_node = new_start_node\n",
        "    new_end_node: Node = Node(is_end=True)\n",
        "    global_graph.add_node(new_end_node)\n",
        "    token1.end_node.add_connection(new_end_node, 'ESP')\n",
        "    token2.end_node.add_connection(new_end_node, 'ESP')\n",
        "    token1.end_node.is_end = False\n",
        "    token2.end_node.is_end = False\n",
        "    token1.end_node = new_end_node\n",
        "    return token1\n",
        "\n",
        "def construct_token(s: str) -> Token:\n",
        "    start_node = Node(is_start=True)\n",
        "    global_graph.add_node(start_node)\n",
        "    end_node = start_node\n",
        "    token = Token(start_node=start_node,end_node=end_node)\n",
        "    index = 0  # Initialize index variable\n",
        "    nesting = False\n",
        "    while index < len(s):\n",
        "        if s[index] == '(':\n",
        "            nesting = True\n",
        "            count = 1\n",
        "            base = \"\"\n",
        "            for index2, char2 in enumerate(s[index + 1:], start=index + 1):\n",
        "              if char2 == '(':\n",
        "                count += 1\n",
        "              if char2 == ')':\n",
        "                  count -= 1\n",
        "                  if count == 0:\n",
        "                      index = index2 + 1\n",
        "                      break\n",
        "              base += char2\n",
        "        elif s[index] == '[':\n",
        "            base = \"[\"\n",
        "            for index2, char2 in enumerate(s[index + 1:], start=index + 1):\n",
        "                if char2 == ']':\n",
        "                    index = index2 + 1  # Move index to the next character after ']'\n",
        "                    base+= char2\n",
        "                    break\n",
        "                base += char2\n",
        "        else:\n",
        "            base = s[index]\n",
        "            index += 1  # Move index to the next character\n",
        "        #Now we have the base, we need to check if there is a quantifier\n",
        "        #if nesting is false then I'm sure the string inside base doesnt need any nesting and can be passed to construct_quantified_base\n",
        "        if (nesting):\n",
        "            #if there is nesting then I need to essentially construct a new token with the string inside the brackets\n",
        "            new_token = construct_token(base)\n",
        "            if (index !=  len(s) ):\n",
        "                #there might be a quantifier after the closing bracket\n",
        "                has_quantifer = False\n",
        "                if (index != len(s) ):\n",
        "                    new_token, has_quantifer = add_quantifier(new_token,s[index])\n",
        "                #if there is a quantifier then I need to increment index\n",
        "                if (has_quantifer): index += 1\n",
        "        else:\n",
        "            #I pass s[index] because it is the char right after the char/range of chars that determine whether the base needs a quantifier or not\n",
        "            new_token = construct_base(base)\n",
        "            has_quantifer = False\n",
        "            if(index != len(s)):\n",
        "                new_token, has_quantifer = add_quantifier(new_token,s[index])\n",
        "            if (has_quantifer): index += 1\n",
        "            #Since the base has a quantifier, I need to check index+1 to see if there is a '|' or not\n",
        "        while index != len(s) and s[index] == '|':\n",
        "            #If there is a '|' then I need to find operand 2 before I can join operand 1 and operand 2\n",
        "            new_token_right = Token()\n",
        "            index += 1\n",
        "            #Well I got 3 cases for the right token/operand. It could be a single character, a range of characters or a nested token\n",
        "            #So I need to check if the next character is a bracket or not\n",
        "            #If it is a bracket then I need to find the closing bracket and pass the string inside the brackets to construct_token\n",
        "            nesting2 = False\n",
        "            if s[index] == '(':\n",
        "                nesting2 = True\n",
        "                count2 = 1\n",
        "                base = \"\"\n",
        "                for index2, char2 in enumerate(s[index + 1:], start=index + 1):\n",
        "                    if char2 == '(':\n",
        "                      count2 += 1\n",
        "                    if char2 == ')':\n",
        "                        count2 -= 1\n",
        "                        if count2 == 0:\n",
        "                          index = index2 + 1  # Move index to the next character after ')'\n",
        "                          break\n",
        "                    base += char2\n",
        "            elif s[index] == '[':\n",
        "                base = \"[\"\n",
        "                for index2, char2 in enumerate(s[index + 1:], start=index + 1):\n",
        "                    if char2 == ']':\n",
        "                        index = index2 + 1  # Move index to the next character after ']'\n",
        "                        base+= char2\n",
        "                        break\n",
        "                    base += char2\n",
        "            else:\n",
        "                base = s[index]\n",
        "                index += 1\n",
        "            if (nesting2):\n",
        "                new_token_right = construct_token(base)\n",
        "                if (index !=  len(s) ):\n",
        "                    new_token_right, has_quantifer = add_quantifier(new_token_right,s[index])\n",
        "                    if (has_quantifer): index += 1\n",
        "            else:\n",
        "                new_token_right = construct_base(base)\n",
        "                has_quantifer = False\n",
        "                if(index != len(s) ):\n",
        "                    new_token_right, has_quantifer = add_quantifier(new_token_right,s[index])\n",
        "                if (has_quantifer): index += 1\n",
        "            new_token = or_tokens(new_token,new_token_right)\n",
        "        token = and_tokens(token,new_token)\n",
        "    return token\n",
        "\n",
        "def correct_token(token: Token) -> Token:\n",
        "    for node in global_graph.nodes:\n",
        "            node.is_end = False\n",
        "    token.end_node.is_end = True\n",
        "    graph_dict = global_graph.to_dict()\n",
        "    token_dict = token.to_dict() #this returns a dictionary containing only the start state\n",
        "    token_dict.update(graph_dict)\n",
        "    return token_dict\n",
        "\n",
        "def part1_assignment(s:str, allowed_non_alnum: Tuple[str, ...]) -> dict:\n",
        "    valid = is_valid_regex(s, allowed_non_alnum)\n",
        "    if valid == RegexErrors.SUCCESS:\n",
        "        print(\"Valid\")\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid regex due to {valid}\")\n",
        "    token: Token = construct_token(s)\n",
        "    token_dict: dict = correct_token(token)\n",
        "    with open(\"NFA.json\", \"w\") as json_file:\n",
        "        json.dump(token_dict, json_file)\n",
        "\n",
        "    return token_dict\n",
        "# Example usage:\n",
        "# global_graph = Graph()\n",
        "# s:str = input()\n",
        "# allowed_non_alnum: Tuple[str, ...] = ('[', ']', '(', ')', '.', '|', '?', '*', '+','-')\n",
        "# token_dict = part1_assignment(s, allowed_non_alnum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8ZaYbH886cU"
      },
      "source": [
        "flow of code\n",
        "    1-convert json into data structure\n",
        "    2-helper function to closure of node\n",
        "    3-transantions table\n",
        "todo list\n",
        "    1-coverting json => Done\n",
        "    2-Get Closure    => Done\n",
        "    3-Define DFA     => Done\n",
        "    4-algorithm   \n",
        "\n",
        "\n",
        "Check List in json    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dw5FqyB86cW"
      },
      "outputs": [],
      "source": [
        "from tokenize import String\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7V93ZTJ86cY"
      },
      "outputs": [],
      "source": [
        "def getClosure(startState, states):\n",
        "    closure = []\n",
        "    closure.append(startState)\n",
        "   #print(startState)\n",
        "   #print(states)\n",
        "    isAddedToClosure=True\n",
        "    while isAddedToClosure:\n",
        "        isAddedToClosure=False\n",
        "        for item in closure:\n",
        "\n",
        "\n",
        "            for key , value in states[item].items():\n",
        "                if key== EPSION:\n",
        "                    if(isinstance(value,list)):\n",
        "                        for valuetemp in value:\n",
        "                          if valuetemp not in closure:\n",
        "                             closure.append(valuetemp)\n",
        "                             isAddedToClosure=True;\n",
        "                    else:\n",
        "                      if value not in closure:\n",
        "                        closure.append(value)\n",
        "                        isAddedToClosure=True;\n",
        "\n",
        "\n",
        "    return closure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADI02JbwPzlm"
      },
      "outputs": [],
      "source": [
        "def applyingFilter(json_data):\n",
        "    for key, value in list(json_data.items()):  # Create a copy of the dictionary for iteration\n",
        "        if key == 'startingState':\n",
        "            continue\n",
        "        for key2, value2 in list(value.items()):  # Create a copy of the dictionary for iteration\n",
        "            if key2[0] == '[':\n",
        "                for i in range(len(key2)):\n",
        "                    if key2[i] == '-':\n",
        "                        for j in range(ord(key2[i-1]), ord(key2[i+1])+1):\n",
        "\n",
        "                            if chr(j) in value and value[chr(j)]==value2  :\n",
        "\n",
        "                                value.pop(chr(j))\n",
        "\n",
        "\n",
        "    return json_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ihcZrLeP5uC"
      },
      "outputs": [],
      "source": [
        "class DFA:\n",
        "    def __init__(self):\n",
        "        self.startState = None\n",
        "        self.acceptStates = []\n",
        "        self.States = []\n",
        "        self.transitions = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3TXg_sPP7NF"
      },
      "outputs": [],
      "source": [
        "class Min_DFA:\n",
        "    def __init__(self):\n",
        "        self.startState = None\n",
        "        self.acceptStates = []\n",
        "        self.States = []\n",
        "        self.transitions = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3Yb1YhQP9En"
      },
      "outputs": [],
      "source": [
        "def CreatingDFA(DFA):\n",
        "  st = []\n",
        "  st.append(DFA.startState)\n",
        "\n",
        "  while st:\n",
        "      transMap = {}\n",
        "      mainState = st.pop()\n",
        "      mainState.sort()\n",
        "      if mainState not in DFA.States:\n",
        "          DFA.States.append(mainState)\n",
        "      isAccept = False\n",
        "      for item in mainState:\n",
        "          for key, value in states[item].items():\n",
        "              if key == \"isTerminatingState\" and value == True:\n",
        "                  isAccept = True\n",
        "              if key != \"isTerminatingState\" and key !=EPSION:\n",
        "                  if key not in transMap:\n",
        "                      transMap[key] = []\n",
        "                  #Check here if the value is a list or a single value\n",
        "                  if isinstance(value,list):\n",
        "                      for i in value:\n",
        "                        transMap[key].append(i)\n",
        "                        value2.extend(getClosure(i, states))\n",
        "                  else:\n",
        "                    transMap[key].append(value)\n",
        "                    value2 = getClosure(value, states)\n",
        "                  for Closureitem in value2:\n",
        "                      transMap[key].append(Closureitem)\n",
        "      if isAccept == True and mainState not in DFA.acceptStates:\n",
        "          DFA.acceptStates.append(mainState)\n",
        "      # Remove duplicates from each list in transMap\n",
        "      for key in transMap:\n",
        "          transMap[key] = list(set(transMap[key]))\n",
        "          transMap[key].sort()\n",
        "          if transMap[key] not in DFA.States:\n",
        "              DFA.States.append(transMap[key])\n",
        "              st.append(transMap[key])\n",
        "          if tuple(mainState) not in DFA.transitions:\n",
        "                  DFA.transitions[tuple(mainState)] = {}\n",
        "\n",
        "          DFA.transitions[tuple(mainState)][key] = transMap[key]\n",
        "  #print(\"DFA States:\", DFA.States)\n",
        "  #print(\"DFA Transitions:\", DFA.transitions)\n",
        "  anotherMap = {}\n",
        "  count = 0\n",
        "  for item in DFA.States:\n",
        "      anotherMap[tuple(item)] = 'S' + str(count)\n",
        "      count += 1\n",
        "  DFA.startState = anotherMap[tuple(DFA.startState)]\n",
        "  # Convert DFA.States\n",
        "  for i in range(len(DFA.States)):\n",
        "      DFA.States[i] = anotherMap[tuple(DFA.States[i])]\n",
        "\n",
        "  # Convert DFA.acceptStates\n",
        "  for i in range(len(DFA.acceptStates)):\n",
        "      DFA.acceptStates[i] = anotherMap[tuple(DFA.acceptStates[i])]\n",
        "\n",
        "  # Convert DFA.transitions\n",
        "  new_transitions = {}\n",
        "  for key, value in DFA.transitions.items():\n",
        "      new_key = anotherMap[tuple(key)]\n",
        "      new_value = {k: anotherMap[tuple(v)] for k, v in value.items()}\n",
        "      new_transitions[new_key] = new_value\n",
        "  DFA.transitions = new_transitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSltFHZLP-xS"
      },
      "outputs": [],
      "source": [
        "#convert DFA to JSON\n",
        "\n",
        "\n",
        "# Create a dictionary that represents the DFA\n",
        "def ConvertDFAtoJSON(DFA):\n",
        "  dfa_dict = {\n",
        "      \"startingState\": DFA.startState,\n",
        "  }\n",
        "\n",
        "  # Add states to the dictionary\n",
        "  for state in DFA.States:\n",
        "      dfa_dict[state] = {\n",
        "          \"isTerminatingState\": state in DFA.acceptStates,\n",
        "      }\n",
        "      # Add transitions for each state\n",
        "      if state in DFA.transitions:\n",
        "          for symbol, nextState in DFA.transitions[state].items():\n",
        "              dfa_dict[state][symbol] = nextState\n",
        "\n",
        "  # Convert the dictionary to a JSON string\n",
        "  dfa_json = json.dumps(dfa_dict, indent=4)\n",
        "  # Specify the file path\n",
        "  file_path = '/content/NFA_To_DFA.json'\n",
        "\n",
        "  # Open the file in write mode\n",
        "  with open(file_path, 'w') as f:\n",
        "      # Write the JSON string to the file\n",
        "      f.write(dfa_json)\n",
        "  dat2 = json.load(open('/content/NFA_To_DFA.json'))\n",
        "  #dat2=applyingFilter(dat2)\n",
        "  DFAstates = {key: value for key, value in dat2.items() if key != 'startingState'}\n",
        "  return DFAstates\n",
        "#print(dfa_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJJp7Ug4QAjy"
      },
      "outputs": [],
      "source": [
        "def CreatingMinDFA(Min_DFA,DFA,DFAstates):\n",
        "  dfa_acceptStates = DFA.acceptStates\n",
        "  dfa_rejectStates=[]\n",
        "  dfa_state_to_min_dfa_state = {}\n",
        "  for state in DFA.States:\n",
        "      if state not in dfa_acceptStates:\n",
        "        dfa_rejectStates.append(state)\n",
        "        dfa_state_to_min_dfa_state[state] = dfa_rejectStates\n",
        "      else:\n",
        "          dfa_state_to_min_dfa_state[state] = dfa_acceptStates\n",
        "  #2 bags created\n",
        "  min_dfa_states = [dfa_acceptStates, dfa_rejectStates]\n",
        "\n",
        "  flag=True\n",
        "  while flag:\n",
        "      flag=False\n",
        "      for bag in min_dfa_states[:]:  # Create a copy for iteration\n",
        "          if(len(bag)>1):\n",
        "              split_map = []\n",
        "              temp_map = {}\n",
        "              for state in bag:\n",
        "                  transsitions_map={}\n",
        "                  for key, value in DFAstates[state].items():\n",
        "                      if key != 'isTerminatingState':\n",
        "                          if value not in transsitions_map:\n",
        "                              transsitions_map[key] = []\n",
        "                          transsitions_map[key].append(value)\n",
        "                  if transsitions_map not in split_map:\n",
        "                      split_map.append(transsitions_map)\n",
        "                      temp_map[state] = [split_map.index(transsitions_map)]  # Make it a list\n",
        "                  else:\n",
        "                      if state not in temp_map:\n",
        "                          temp_map[state] = []\n",
        "                      temp_map[state].append(split_map.index(transsitions_map))  # Use append instead of add\n",
        "              if len(split_map)>1:\n",
        "                  flag=True\n",
        "                  temp_map2 = {}\n",
        "                  for key,value in temp_map.items():  # Iterate over items, not the dictionary itself\n",
        "                      if tuple(value) not in temp_map2:\n",
        "                          temp_map2[tuple(value)] = []\n",
        "                      temp_map2[tuple(value)].append(key)\n",
        "                  for value in temp_map2.values():  # Iterate over the values, not the keys\n",
        "                      min_dfa_states.append(value)\n",
        "                      for state in value:\n",
        "                          dfa_state_to_min_dfa_state[state] = value\n",
        "                  min_dfa_states.remove(bag)\n",
        "  #print(min_dfa_states)\n",
        "  #print(dfa_state_to_min_dfa_state)\n",
        "  Min_DFA.startState = dfa_state_to_min_dfa_state[DFA.startState]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  anotherMap2 = {}\n",
        "  count2 = 0\n",
        "  for key,value in dfa_state_to_min_dfa_state.items():\n",
        "      if tuple(value) not in anotherMap2:\n",
        "          anotherMap2[tuple(value)] = 'S' + str(count2)\n",
        "          count2 += 1\n",
        "\n",
        "  Min_DFA.startState=anotherMap2[tuple(Min_DFA.startState)]\n",
        "  for state in DFA.acceptStates:\n",
        "      Min_DFA.acceptStates.append(anotherMap2[tuple(dfa_state_to_min_dfa_state[state])])\n",
        "\n",
        "  for item in dfa_state_to_min_dfa_state:\n",
        "      if anotherMap2[tuple(dfa_state_to_min_dfa_state[item])] not in Min_DFA.States:\n",
        "          Min_DFA.States.append(anotherMap2[tuple(dfa_state_to_min_dfa_state[item])])\n",
        "\n",
        "\n",
        "  # # Convert DFA.transitions\n",
        "  new_transitions2 = {}\n",
        "  for key, value in DFA.transitions.items():\n",
        "      new_key = anotherMap2[tuple(dfa_state_to_min_dfa_state[key])]\n",
        "      new_value = {}  # Initialize an empty dictionary\n",
        "      for k, v in value.items():\n",
        "          tuple_key = tuple(dfa_state_to_min_dfa_state[v])\n",
        "          anotherMap2_value = anotherMap2[tuple_key]\n",
        "          new_value[k] = anotherMap2_value\n",
        "      new_transitions2[new_key] = new_value\n",
        "  Min_DFA.transitions = new_transitions2\n",
        "\n",
        "  #print(new_transitions2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1Ql_WIoQBzW"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary that represents the DFA\n",
        "def MinDFAToJson(Min_DFA):\n",
        "  dfa_dict2 = {\n",
        "      \"startingState\": Min_DFA.startState,\n",
        "  }\n",
        "\n",
        "  # Add states to the dictionary\n",
        "  for state in Min_DFA.States:\n",
        "      dfa_dict2[state] = {\n",
        "          \"isTerminatingState\": state in Min_DFA.acceptStates,\n",
        "      }\n",
        "      # Add transitions for each state\n",
        "      if state in Min_DFA.transitions:\n",
        "          for symbol, nextState in Min_DFA.transitions[state].items():\n",
        "              dfa_dict2[state][symbol] = nextState\n",
        "\n",
        "  # Convert the dictionary to a JSON string\n",
        "  dfa_json2 = json.dumps(dfa_dict2, indent=4)\n",
        "  # Specify the file path\n",
        "  file_path = '/content/DFA.json'\n",
        "\n",
        "  # Open the file in write mode\n",
        "  with open(file_path, 'w') as f:\n",
        "      # Write the JSON string to the file\n",
        "      f.write(dfa_json2)\n",
        "  dat22 = json.load(open('/content/DFA.json'))\n",
        "  #dat22=applyingFilter(dat22);\n",
        "  MinDFAstates = {key: value for key, value in dat22.items() if key != 'startingState'}\n",
        "\n",
        "\n",
        "  return MinDFAstates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCMgNublQDGw"
      },
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def vizualize_DFA(Min_DFA,MinDFAstates):\n",
        "\n",
        "  #gra = Digraph(graph_attr={'landscape':'True'})\n",
        "  gra = Digraph(graph_attr={'rankdir':'LR'})\n",
        "\n",
        "\n",
        "  #construct nodes first\n",
        "  for stat in Min_DFA.States:\n",
        "     for key, value in MinDFAstates[stat].items():\n",
        "        if key == 'isTerminatingState':\n",
        "          if value==True:\n",
        "            gra.node(stat, _attributes={'peripheries' : '2'})\n",
        "          else:\n",
        "            gra.node(stat)\n",
        "\n",
        "  #for each node, construct edges\n",
        "  for stat in Min_DFA.States:\n",
        "      for edg,value in MinDFAstates[stat].items():\n",
        "          if(edg !=\"isTerminatingState\"):\n",
        "            gra.edge(stat, value, edg)\n",
        "\n",
        "  gra.node('', shape='none')\n",
        "  gra.edge('',Min_DFA.startState, label='Start')\n",
        "  gra.format = 'png'\n",
        "  gra.render('DFA', view = True)\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Twg6xdD_QEFF"
      },
      "outputs": [],
      "source": [
        "global_graph = Graph()\n",
        "s:str = input()\n",
        "allowed_non_alnum: Tuple[str, ...] = ('[', ']', '(', ')', '.', '|', '?', '*', '+','-')\n",
        "token_dict = part1_assignment(s, allowed_non_alnum)\n",
        "vizualize_NFA(token_dict)\n",
        "\n",
        "\n",
        "\n",
        "with open('/content/NFA.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Print the data\n",
        "#print(data['startingState'])\n",
        "startState = data['startingState'];\n",
        "states = {key: value for key, value in data.items() if key != 'startingState'}\n",
        "\n",
        "DFA = DFA()\n",
        "DFA.startState = getClosure(startState, states)\n",
        "CreatingDFA(DFA)\n",
        "DFAstates=ConvertDFAtoJSON(DFA)\n",
        "Min_DFA = Min_DFA()\n",
        "CreatingMinDFA(Min_DFA,DFA,DFAstates)\n",
        "MinDFAstates=MinDFAToJson(Min_DFA)\n",
        "vizualize_DFA(Min_DFA,MinDFAstates)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}